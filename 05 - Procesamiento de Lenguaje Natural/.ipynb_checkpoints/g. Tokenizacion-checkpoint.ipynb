{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<img src=\"./Imagenes/ITESO_Logo.png\" style=\"width:500px;height:142px;\" title=\"Logo ITESO\">\n",
    "<br><font face = \"Times New Roman\" size = \"6\"><b><center>Maestría en Sistemas Computacionales</center></b></font>\n",
    "<br><font face = \"Times New Roman\" size = \"5\"><b><center>Programación para Análisis de Datos</center></b></font>\n",
    "\n",
    "<b><br><font face = \"Times New Roman\" size = \"4\"><center>Unidad 5: Proceso de Selección de Métodos</center></font>\n",
    "<font face = \"Times New Roman\" size = \"4\"><center>Tema 5.1: Procesamiento de Lenguaje Natural</center></font>\n",
    "<font face = \"Times New Roman\" size = \"4\"><center>Subtema g: Tokenización</center></font></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZACIÓN\n",
    "\n",
    "**Tokenización** es el proceso de \"romper\" una cadena de texto a una lista de palabras y símbolos de puntuación, de manera similar a lo revisado en el ***Subtema e***.\n",
    "\n",
    "Los pasos a seguir son:\n",
    "\n",
    "1. Se importará la librería **NLTK**.\n",
    "2. Se creará una nueva cadena de texto.\n",
    "3. Se realizará la **Tokenización** de la nueva cadena de texto.\n",
    "4. Se muestran los ***tokens*** contenidos en la cadena de texto.\n",
    "\n",
    "Esto es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los \"tokens\" contenidos en el enunciado son:\n",
      " ['Estoy', 'aprendiendo', 'Procesamiento', 'de', 'Lenguaje', 'Natural', '.']\n"
     ]
    }
   ],
   "source": [
    "#Importación de la librería NLTK\n",
    "import nltk\n",
    "\n",
    "#Creación de una cadena de texto\n",
    "texto = \"Estoy aprendiendo Procesamiento de Lenguaje Natural.\"\n",
    "\n",
    "#Tokenización de la cadena de texto\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "\n",
    "#Impresión de los Resultados\n",
    "print(\"Los \\\"tokens\\\" contenidos en el enunciado son:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible obtener la cantidad de ***tokens*** presentes en la cadena de texto por medio de la función `len()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de tokens en el enunciado es: 7\n"
     ]
    }
   ],
   "source": [
    "#Determinación de la cantidad de tokens\n",
    "print(\"La cantidad de tokens en el enunciado es:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible realizar la **Tokenización** no solo por palabra sino por enunciado. Para ello se empleando el método `sent_tokenize()` de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los \"tokens\" por enunciado contenidos en la frase son:\n",
      " ['Estoy aprendiendo Procesamiento de Lenguaje Natural.', '¡Aprendo a tokenizar!']\n"
     ]
    }
   ],
   "source": [
    "#Creación de una frase de texto\n",
    "frase = \"Estoy aprendiendo Procesamiento de Lenguaje Natural. ¡Aprendo a tokenizar!\"\n",
    "\n",
    "#Tokenización de la frase de texto en enunciados\n",
    "tokens_sent = nltk.sent_tokenize(frase)\n",
    "\n",
    "#Impresión de los Resultados\n",
    "print(\"Los \\\"tokens\\\" por enunciado contenidos en la frase son:\\n\", tokens_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible obtener la cantidad de tokens presentes en la cadena de texto por medio de la función `len()`, en este caso mostrará la cantidad de enunciados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de enunciados en la frase es: 2\n"
     ]
    }
   ],
   "source": [
    "#Determinación de la cantidad de tokens\n",
    "print(\"La cantidad de enunciados en la frase es:\", len(tokens_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esa manera podemos tokenizar las palabras presentes en cada enunciado por medio de un ciclo `for`de la siguiete manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Estoy', 'aprendiendo', 'Procesamiento', 'de', 'Lenguaje', 'Natural', '.']\n",
      "['¡Aprendo', 'a', 'tokenizar', '!']\n"
     ]
    }
   ],
   "source": [
    "#Tokenización de las palabras en los enunciados\n",
    "for item in tokens_sent:\n",
    "    print(nltk.word_tokenize(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>.: Fin del Tema :.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
